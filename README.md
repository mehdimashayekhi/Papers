# Good-Papers/Talks, some of my recent readings/learnings

List of papers 

## Natural Language Understanding Related Papers

* [StarSpace: Embed All The Things!] (https://arxiv.org/pdf/1709.03856.pdf)
* [ConvS2S] (https://arxiv.org/abs/1705.03122)
* [Attention in general] (https://arxiv.org/pdf/1703.03906.pdf) (the link would help you quickly understand section 3.2)
*  Wavenet/Bytenet https://arxiv.org/abs/1609.03499 and https://arxiv.org/abs/1610.10099
*  Intra-attention: https://arxiv.org/pdf/1705.04304.pdf
* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) - Attention Is All You Need. <br />Presentation: https://www.youtube.com/watch?v=rBCqOTEfxvg. <br /> Code: https://github.com/tensorflow/tensor2tensor
* [Grammar as a Foreign Language](https://arxiv.org/pdf/1412.7449.pdf)
* [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](https://arxiv.org/pdf/1409.0473.pdf)
* [OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER](https://arxiv.org/pdf/1701.06538.pdf)
* [Tensor2Tensor Presentation] (https://nlp.stanford.edu/seminar/details/lkaiser.pdf)
* [https://arxiv.org/pdf/1707.05589.pdf] ON THE STATE OF THE ART OF EVALUATION IN NEURAL LANGUAGE MODELS
* [https://www.youtube.com/user/neubig/videos] CMU CS 11-747, Neural Networks for NLP 

* [https://arxiv.org/pdf/1603.01354.pdf] End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF 

* [Summary of NIPS 2017 conference] https://cs.brown.edu/~dabel/blog/posts/misc/nips_2017.pdf

* [LEARNING DOCUMENT EMBEDDINGS BY PREDICTING N-GRAMS FOR SENTIMENT CLASSIFICATION OF LONG MOVIE REVIEWS] https://arxiv.org/pdf/1512.08183.pdf

* [One Model To Learn Them All] (https://arxiv.org/pdf/1706.05137.pdf)

* [EASY CONTEXTUAL INTENT PREDICTION AND SLOT DETECTION] (http://www.cs.toronto.edu/~aditya/publications/contextual.pdf)

* [Language Modeling with Gated Convolutional Networks, 2017] (https://arxiv.org/pdf/1612.08083.pdf)

* [End-to-End Memory Networks with Knowledge Carryover for Multi-Turn Spoken Language Understanding, 2017] (https://www.csie.ntu.edu.tw/%7Eyvchen/doc/IS16_ContextualSLU.pdf), https://github.com/yvchen/ContextualSLU

- Georgia Tech的免费书：https://lnkd.in/e6VSDXy
- Yoav Goldberg新出的书：https://lnkd.in/eZra8Xb
- Stanford - CS224n：https://lnkd.in/eTtF5Ju
- CMU的课：https://lnkd.in/euf622B
- University of Oxford and DeepMind的课：https://lnkd.in/exvG_s2

* [Named Entity Recognition with Bidirectional LSTM-CNNs (adding-lexicon-to-nlu), 2016] (https://arxiv.org/pdf/1511.08308v4.pdf)

* [Neural Belief Tracker: Data-Driven Dialogue State Tracking, 2017] (https://arxiv.org/pdf/1606.03777.pdf)

* [FRAMES: A CORPUS FOR ADDING MEMORY TO GOAL-ORIENTED DIALOGUE SYSTEMS, 2017] (https://arxiv.org/pdf/1704.00057.pdf)

* [END-TO-END JOINT LEARNING OF NATURAL LANGUAGE UNDERSTANDING AND DIALOGUE MANAGER] (https://arxiv.org/pdf/1612.00913.pdf)

## (Deep) Reinforcement Learning
* [Deep Reinforcement Learning Tutorial (Karpathy 2016)]http://karpathy.github.io/2016/05/31/rl/
* [Reinforcement Learning: An Introduction, Richard S. Sutton,2017] http://incompleteideas.net/book/bookdraft2017nov5.pdf
* [Reinforcement Learning: An Introduction, Richard S. Sutton,2018] https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view
* [REINFORCE (Williams 1992)]http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf
* [Co-training (Blum and Mitchell 1998)]http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/avrim/Papers/cotrain.pdf
* [Adding Baselines (Dayan 1990)]http://www.gatsby.ucl.ac.uk/~dayan/papers/reinfss.pdf
* [Sequence-level Training for RNNs (Ranzato et al. 2016)]https://arxiv.org/pdf/1511.06732.pdf
* [Experience Replay (Lin 1993)]http://www.dtic.mil/docs/citations/ADA261434
* [Neural Q Learning (Tesauro 1995)]https://cling.csd.uwo.ca/cs346a/extra/tdgammon.pdf
* [Intrinsic Reward (Schmidhuber 1991)]https://pdfs.semanticscholar.org/2980/dfe5c99658dc3e508d9d6e1d7f26e6fc8934.pdf
* [Intrinsic Reward for Atari (Bellemare et al. 2016)]http://papers.nips.cc/paper/6383-unifying-count-based-exploration-and-intrinsic-motivation.pdf
* [Reinforcement Learning for Dialog (Young et al. 2013)]http://mi.eng.cam.ac.uk/~sjy/papers/ygtw13.pdf
* [End-to-end Neural Task-based Dialog (Williams and Zweig 2016)]https://arxiv.org/pdf/1606.01269.pdf
* [Neural Chat Dialog (Li et al. 2016)]http://anthology.aclweb.org/D/D16/D16-1127.pdf
* [User Simulation for Learning in Dialog (Schatzmann et al. 2007)]http://www.aclweb.org/anthology/N/N07/N07-2038.pdf
* [RL for Mapping Instructions to actions (Branavan et al. 2009)]http://www.anthology.aclweb.org/P/P09/P09-1010.pdf
* [Deep RL for Mapping Instructions to Actions (Misra et al. 2017)]https://www.aclweb.org/anthology/D/D17/D17-1107.pdf
* [RL for Text-based Grames (Narasimhan et al. 2015)]http://aclweb.org/anthology/D15-1001.pdf
* [Incremental Prediction in MT (Grissom et al. 2014)]http://www.aclweb.org/anthology/D14-1140
* [Incremental Neural MT (Gu et al. 2017)]http://www.aclweb.org/anthology/E/E17/E17-1099.pdf
* [RL for Information Retrieval (Narasimhan et al. 2016)]http://aclweb.org/anthology/D/D16/D16-1261.pdf
* [RL for Query Reformulation (Nogueira and Cho 2017)]http://aclweb.org/anthology/D/D17/D17-1062.pdf
* [RL for Coarse-to-fine Question Answering (Choi et al. 2017)]http://aclweb.org/anthology/P/P17/P17-1020.pdf
* [RL for Learning Neural Network Structure (Zoph and Le 2016)]https://arxiv.org/pdf/1611.01578.pdf
* [Sample Code: Reinforcement Learning Code Examples]https://github.com/neubig/nn4nlp-code
* [Course CMU CS 11-747, Neural Network for NLP, Fall 2017]http://www.phontron.com/class/nn4nlp2017/schedule.html
* [Deep RL Bootcamp, 26-27 August 2017, Berkeley CA] https://sites.google.com/view/deep-rl-bootcamp/lectures
* [Deep Reinforcement Learning for Dialogue Generation] https://arxiv.org/pdf/1606.01541.pdf
* [Adversarial Learning for Neural Dialogue Generation] https://arxiv.org/pdf/1701.06547.pdf
* [Deal or No Deal? End-to-End Learning for Negotiation Dialogues]https://arxiv.org/pdf/1706.05125.pdf,        [Presentation]https://nlp.stanford.edu/seminar/details/mlewis.pdf
* [CS 294: Deep Reinforcement Learning, Fall 2017] http://rll.berkeley.edu/deeprlcourse/
* https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html
* https://leimao.github.io/article/Flappy-Bird-AI/
* [Pieter Abbeel delivers his keynote: Deep Learning for Robotics, at NIPS 2017.]https://www.facebook.com/nipsfoundation/videos/1554594181298482/
* Meta-Learning https://www.dropbox.com/s/a82gbu55k1k4diz/2017_12_xx_NIPS-HRL-workshop-final.pdf?dl=0
* [Policy Networks with Two-Stage Training for Dialogue Systems] https://arxiv.org/pdf/1606.03152.pdf
* [Seris of Blogs implementaion of RL] https://jaromiru.com/2016/10/21/lets-make-a-dqn-full-dqn/

## Some Interesting Machine Learning Comprehension (MRC) papers/Talks
* [ReasoNet: Learning to Stop Reading in Machine Comprehension, 2017] https://arxiv.org/pdf/1609.05284.pdf
* [S-NET: FROM ANSWER EXTRACTION TO ANSWER GENERATION FOR MACHINE READING COMPREHEN- SION, 2018] https://arxiv.org/pdf/1706.04815.pdf
* [Machine Reading Using Neural Machines, Microsfot Research, 2017] https://www.youtube.com/watch?v=73xYpRKuZVI


## Tensorflow Tutorials

* https://github.com/nlintz/TensorFlow-Tutorials
* https://github.com/pkmital/tensorflow_tutorials
* https://github.com/aymericdamien/TensorFlow-Examples



## Some interesting ML talks 

* [Ewa Dominowska - Generating a Billion Personal News Feeds - MLconf SEA 2016] https://www.youtube.com/watch?v=iXKR3HE-m8c
* [Rushin Shah, Engineering Manager, Facebook, NLP Related] https://www.youtube.com/watch?v=avViRGkdVKY



## Multi-Dimensional Recurrent Neural Networks

* [Multi-Dimensional Recurrent Neural Networks, 2013] https://arxiv.org/pdf/0705.2011.pdf
* [Generative Image Modeling Using Spatial LSTMs, 2015] https://arxiv.org/pdf/1506.03478.pdf
* [Pixel Recurrent Neural Networks, 2016] https://arxiv.org/pdf/1601.06759.pdf
* [Generative Models(GAN, PixelRNN,..., 2017)] (https://www.youtube.com/watch?v=5WoItGTWV54)
* [PIXELCNN++: IMPROVING THE PIXELCNN WITH DISCRETIZED LOGISTIC MIXTURE LIKELIHOOD AND OTHER MODIFICATIONS), 2017] (https://arxiv.org/pdf/1701.05517.pdf)
* https://www.youtube.com/watch?v=OsunRTEh1pw&index=5&list=PLdk2fd27CQzSd1sQ3kBYL4vtv6GjXvPsE, Variational auto encoder


## Yes I GAN!

* [NIPS 2016 - Generative Adversarial Networks - Ian Goodfellow] https://www.youtube.com/watch?v=AJVyzd0rqdc, https://arxiv.org/pdf/1701.00160.pdf

* [Least Squares Generative Adversarial Networks, 2017] https://arxiv.org/pdf/1611.04076.pdf

* https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html

* https://towardsdatascience.com/gan-introduction-and-implementation-part1-implement-a-simple-gan-in-tf-for-mnist-handwritten-de00a759ae5c

* [UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS, DCGAN, 2016] https://github.com/Newmu/dcgan_code, https://arxiv.org/pdf/1511.06434.pdf

* [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, 2016] https://arxiv.org/pdf/1606.03657.pdf


## Localization and Object Detection

* [OverFeat:Integrated Recognition, Localization and Detection using Convolutional Networks, 2014] https://arxiv.org/abs/1312.6229

## Docker

* Using Jupyter notebook from Docker: https://www.dataquest.io/blog/docker-data-science/
* General introduction to Docker: https://docker-curriculum.com/





